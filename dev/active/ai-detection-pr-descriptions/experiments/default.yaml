# AI Detection Experiment Configuration
# See RUNBOOK-EXPERIMENTS.md for usage instructions

experiment:
  name: "ai-detection-baseline"
  description: "Baseline experiment for AI detection using Groq Llama 3.3 70B"
  version: "1.0"

# Model configuration
model:
  provider: "groq"
  name: "llama-3.3-70b-versatile"
  temperature: 0
  max_tokens: 500
  # Batch API settings
  batch:
    enabled: true
    completion_window: "24h"

# Prompt configuration
prompt:
  file: "prompts/v1.md"
  # Response schema enforced via Groq JSON mode
  response_schema:
    type: "object"
    properties:
      is_ai_assisted:
        type: "boolean"
      tools:
        type: "array"
        items:
          type: "string"
      usage_category:
        type: "string"
        enum: ["authored", "assisted", "reviewed", "brainstorm", null]
      confidence:
        type: "number"
      reasoning:
        type: "string"
    required: ["is_ai_assisted", "tools", "confidence"]

# Target repositories
repos:
  # Tier 1: High signal
  - owner: antiwork
    repo: gumroad
    tier: 1
    limit: 100
    notes: "PRIMARY - 68% have AI Disclosure section"

  - owner: anthropic
    repo: anthropic-cookbook
    tier: 1
    limit: 50
    notes: "Claude creator"

  # Tier 3: Negative examples
  - owner: django
    repo: django
    tier: 3
    limit: 50
    notes: "Traditional OSS for calibration"

# Logging configuration
logging:
  # PostHog LLM Analytics
  posthog:
    enabled: true
    host: "https://app.posthog.com"
    # API key from environment: POSTHOG_API_KEY

    # Custom properties added to every $ai_generation event
    properties:
      experiment_version: "1.0"
      detection_type: "ai_pr_detection"

  # Local file logging
  local:
    enabled: true
    output_dir: "experiments/results"
    formats:
      - "json"      # Full results
      - "csv"       # Summary for spreadsheet

  # Console output
  console:
    enabled: true
    verbosity: "normal"  # quiet, normal, verbose

# Evaluation settings
evaluation:
  # Ground truth file (optional)
  ground_truth_file: null

  # Metrics to compute
  metrics:
    - detection_rate
    - precision
    - recall
    - f1_score
    - agreement_rate

  # Compare with regex baseline
  compare_regex: true

  # Thresholds
  confidence_threshold: 0.7
  min_sample_size: 20

# Processing settings
processing:
  # Rate limiting
  requests_per_minute: 30
  retry_attempts: 3
  retry_delay_seconds: 5

  # Batching
  batch_size: 100
  max_concurrent: 5

  # Error handling
  on_error: "log_and_continue"  # or "stop"
