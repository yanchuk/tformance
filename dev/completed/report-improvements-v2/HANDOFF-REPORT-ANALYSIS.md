# AI Impact Report Analysis - Handoff Document

**Date:** 2025-12-27 (Updated)
**Status:** Phase 4 Complete - OSS data updated to 100+ companies, 167k PRs
**Report URL:** https://tformance.com (GitHub Pages: `/docs/index.html`)

---

## Executive Summary

This document captures the analysis and implementation of the Code AI vs Review AI categorization for Tformance's AI Impact Research Report. The key finding is that **Review AI tools (CodeRabbit, Cubic) drive efficiency gains, while Code AI tools show mixed results**.

### Key Numbers (Verified - Updated 2025-12-27)

| Metric | Value | Source |
|--------|-------|--------|
| OSS Companies | 101 (100 in marketing) | `overall_stats.txt` |
| Total PRs | 167,308 | `overall_stats.txt` |
| Companies with 500+ PRs | 74 | `overall_stats.txt` |
| AI Adoption Rate | 12.1% (20,260 PRs) | `overall_stats.txt` |
| LLM Analyzed | 109,240 (65.3%) | `overall_stats.txt` |

### Category Breakdown

| Category | Count | Percentage | Primary Tools |
|----------|-------|------------|---------------|
| **Review AI** | 18,534 | 73.5% | CodeRabbit (11,104), Cubic (6,943), Greptile (469) |
| **Code AI** | 6,625 | 26.3% | Devin (1,795), Cursor (1,429), Claude (1,229), Copilot (823) |
| Unknown | 50 | 0.2% | Mintlify, Snyk |

### Impact by Category (vs Non-AI Baseline)

| Category | PRs | Cycle Time | Review Time | Avg Size |
|----------|-----|------------|-------------|----------|
| **None (baseline)** | 111,200 | 82.2 hrs | 40.8 hrs | 723 lines |
| **Code AI** | 3,056 | **+16%** (95.6 hrs) | **-14%** (34.9 hrs) | 519 lines |
| **Review AI** | 11,302 | **-11%** (73.4 hrs) | **-54%** (18.7 hrs) | 481 lines |

---

## Critical Insights

### 1. Review AI is the Clear Efficiency Win

**Finding:** Review AI tools deliver measurable time savings:
- 11% faster cycle time
- 54% faster review time

**Hypothesis:** Automated code reviews (CodeRabbit, Cubic) replace some human review time while catching issues early. The bot reviews provide immediate feedback, reducing back-and-forth.

### 2. Code AI Shows Mixed Results

**Finding:** Code AI tools increase cycle time but reduce review time:
- +16% cycle time (slower overall)
- -14% review time (faster reviews once code is ready)

**Hypothesis:** AI-generated code may require more iteration/refinement before it's review-ready. The code is generated quickly but needs human validation and adjustment.

### 3. The Detection Bias is Real

**Finding:** Our 12.1% detected AI adoption is a **floor**, not a ceiling.

**Why the Gap Exists:**
- Industry surveys show 84% AI adoption (Stack Overflow 2025)
- We only detect **explicit** AI mentions in PRs
- Copilot autocomplete leaves **no trace** (we detected only 823 mentions despite 68% survey adoption)
- ChatGPT research leaves **no trace** (455 mentions despite 82% survey use)

**Implication:** Review AI dominates our detection because bots leave visible comments. Code AI is massively underrepresented.

### 4. METR RCT Alignment

**Finding:** Our behavioral data aligns with the only randomized controlled trial in the space.

- METR 2025 found AI makes devs **19% slower** (controlled study)
- Developers **believed** they were 20% faster (43-point perception gap)
- Our Code AI data shows **+16% cycle time** â€” similar direction

**Implication:** Survey data reflects perception, not reality. Behavioral data and RCT align.

---

## Files Modified

### Export Script (`docs/scripts/export_report_data.py`)

**New Functions Added:**

1. `export_ai_categories()` - Lines 281-350
   - Categorizes tools using `apps/metrics/services/ai_categories.py`
   - Outputs: `ai_categories.csv`, `ai_tools_with_categories.csv`

2. `export_category_metrics()` - Lines 353-476
   - Calculates cycle/review time by AI category
   - Compares against non-AI baseline
   - Outputs: `category_metrics.csv`

### Report HTML (`docs/index.html`)

**Sections Updated:**

1. **Executive Summary** (line ~1657)
   - Updated: 161,925 PRs, 74 teams, 12.1% adoption, -50% review time, -9% cycle time
   - Added nuanced insight about Review AI vs Code AI

2. **Statistical Confidence** (line ~1525)
   - Updated: 161,925 PRs, Â±0.16% CI, 12.1% adoption
   - Added category-specific findings to "What We Can Confidently Say"

3. **Code AI vs Review AI Section** (line ~1737, id="ai-categories")
   - Added real category breakdown: 73.5% Review, 26.3% Code
   - Added detection bias warning
   - Added Category Impact Comparison (3-column layout)
   - Added insights with data backing
   - Added CTO-focused Key Takeaway

4. **Methodology/Data Quality** (line ~2439)
   - Updated: 161,925 PRs, 107,071 LLM analyzed, 74 teams

5. **Industry Context** (line ~2458)
   - Updated: 12.1% detected adoption with 161k PRs context

6. **Various References**
   - Updated "51 teams" â†’ "74 teams" in multiple places

### Data Files Created (`docs/data/`)

| File | Contents |
|------|----------|
| `ai_categories.csv` | Category totals and percentages |
| `ai_tools_with_categories.csv` | 58 tools with category assignments |
| `category_metrics.csv` | Impact metrics by category |
| `overall_stats.txt` | Summary statistics |

---

## Data Validation

### Sanity Checks Performed

1. **Total PR count matches**
   - Export: 161,925
   - Report: 161,925 âœ“

2. **Category percentages sum to 100%**
   - 73.5% + 26.3% + 0.2% = 100% âœ“

3. **Tool counts match categories**
   - Review: 11,104 + 6,943 + 469 + 16 + 2 = 18,534 âœ“
   - Code: 1,795 + 1,429 + ... = 6,625 âœ“

4. **Impact deltas are mathematically correct**
   - Code AI cycle: (95.6 - 82.2) / 82.2 = 16.3% â‰ˆ 16% âœ“
   - Review AI cycle: (73.4 - 82.2) / 82.2 = -10.7% â‰ˆ -11% âœ“

5. **HTML is well-formed**
   - Validated with Python HTML parser âœ“
   - File size: 178,894 bytes, 3,660 lines âœ“

---

## Outstanding Work (Phase 5-6)

### Phase 5: Quality Metrics (0/3 complete)

- [ ] Revert rate by AI category
- [ ] Hotfix rate by AI category
- [ ] Review friction analysis (rounds, comments)

**Why Important:** Would provide evidence on whether AI code has more defects.

### Phase 6: Visual Polish (1/3 complete)

- [x] Warning/caveat components
- [ ] Category color scheme (consistent orange/teal)
- [ ] Mobile responsiveness check

---

## Competitive Positioning

### Our Unique Differentiator

**No other report has Code AI vs Review AI categorization.**

| Competitor | What They Measure | Our Advantage |
|------------|-------------------|---------------|
| Greptile | Ecosystem trends (PyPI/npm downloads) | PR-level behavioral data |
| CodeRabbit | Quality metrics (issues/PR) | Team-specific insights |
| Stack Overflow | Developer surveys | Actual behavior vs perception |
| JetBrains | Developer surveys | Behavioral measurement |
| METR | RCT (gold standard) | Scale (161k PRs vs 246 issues) |

### Key Messaging

1. **"Behavioral Reality"** - We measure what happens, not what developers say
2. **"Floor vs Ceiling"** - 12% is minimum detected, not maximum actual
3. **"Review AI Wins"** - Clear efficiency gains with Review AI tools
4. **"Code AI Caution"** - Mixed results suggest targeted use cases

---

## Technical Implementation Details

### AI Category Logic

Located in `apps/metrics/services/ai_categories.py`:

```python
CODE_AI_TOOLS = [
    "cursor", "copilot", "claude", "claude_code", "chatgpt",
    "gemini", "windsurf", "codeium", "devin", "aider", ...
]

REVIEW_AI_TOOLS = [
    "coderabbit", "greptile", "cubic", "sourcery", "codacy", ...
]

def get_tool_category(tool: str) -> str | None:
    """Returns: "code" | "review" | None"""
```

### Detection Priority

LLM-detected tools take priority over regex patterns:

```python
# From PullRequest model
@property
def effective_ai_tools(self) -> list[str]:
    if self.llm_summary:
        llm_tools = self.llm_summary.get("ai", {}).get("tools", [])
        if llm_tools:
            return llm_tools
    return self.ai_tools_detected or []
```

### Database Query Pattern

The export script uses raw SQL for performance:

```sql
SELECT tool, COUNT(*) as count
FROM metrics_pullrequest pr
CROSS JOIN LATERAL jsonb_array_elements_text(
    COALESCE(llm_summary->'ai'->'tools', ai_tools_detected)
) as tool
WHERE pr.pr_created_at >= '2025-01-01'
  AND pr.team_id = ANY(team_ids)
GROUP BY tool
ORDER BY count DESC
```

---

## Running the Export

```bash
cd /Users/yanchuk/Documents/GitHub/tformance
.venv/bin/python docs/scripts/export_report_data.py
```

Output:
```
============================================================
Report Data Export - 2025-12-27 18:48
============================================================
Configuration: MIN_PRS_THRESHOLD=500, YEAR=2025

  Summary: 74 teams, 161,925 PRs, 12.1% AI adoption
  Category breakdown: Code=26.3%, Review=73.5%
  Category metrics: Code AI 16% cycle time
                    Review AI -11% cycle time

ðŸ“Š Category Impact Summary:
  Code AI: +16% cycle time, -14% review time
  Review AI: -11% cycle time, -54% review time
```

---

## Questions for Future Work

1. **Should we segment Code AI further?**
   - Agents (Devin, Cubic) vs Human-Directed (Cursor, Claude)
   - May reveal different patterns

2. **Is the +16% Code AI cycle time causal?**
   - Could be selection bias (AI used on complex work)
   - Could be iteration overhead (generate â†’ review â†’ revise)

3. **Why is Review AI so dominant (73.5%)?**
   - Detection bias (bots leave comments)
   - Or actual adoption (review bots are easier to deploy org-wide)

4. **Should we add quality metrics?**
   - Revert rate would validate/invalidate AI code quality concerns
   - May align with CodeRabbit's "1.7x more issues" finding

---

## Appendix: Tool Category Mapping

### Top 20 Tools by Detection Count

| Rank | Tool | Count | Category |
|------|------|-------|----------|
| 1 | coderabbit | 11,104 | review |
| 2 | cubic | 6,943 | review |
| 3 | devin | 1,795 | code |
| 4 | cursor | 1,429 | code |
| 5 | claude | 1,229 | code |
| 6 | copilot | 823 | code |
| 7 | greptile | 469 | review |
| 8 | chatgpt | 455 | code |
| 9 | claude_code | 310 | code |
| 10 | gemini | 196 | code |
| 11 | ai_generic | 159 | code |
| 12 | ellipsis | 98 | code |
| 13 | codegen | 33 | code |
| 14 | mintlify | 25 | unknown |
| 15 | sourcery | 16 | review |
| 16 | continue | 11 | code |
| 17 | aider | 10 | code |
| 18 | replexica | 9 | code |
| 19 | bito | 8 | code |
| 20 | snyk | 8 | unknown |

---

*Document generated for handoff purposes. All data verified as of 2025-12-27.*
