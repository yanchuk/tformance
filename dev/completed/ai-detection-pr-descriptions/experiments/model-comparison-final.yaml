# Final Model Comparison: GPT-OSS-20B vs Llama 3.3 70B
# Key fixes: include_reasoning: false, direct variable substitution
# Run: GROQ_API_KEY=xxx npx promptfoo eval -c model-comparison-final.yaml

description: "Final Model Comparison: 50 PRs"

providers:
  - id: groq:llama-3.3-70b-versatile
    label: "Llama-3.3-70B"
    config:
      temperature: 0
      max_tokens: 800
      response_format:
        type: json_object

  - id: groq:openai/gpt-oss-20b
    label: "GPT-OSS-20B"
    config:
      temperature: 0
      max_tokens: 800
      include_reasoning: false
      response_format:
        type: json_object

# Direct variable substitution - no nested {{user_prompt}}
prompts:
  - id: v6.2.0
    raw: |
      [
        {"role": "system", "content": "{{system_prompt}}"},
        {"role": "user", "content": "Analyze this pull request:\n\nDescription:\n{{pr_body}}"}
      ]

defaultTest:
  vars:
    system_prompt: file://prompts/v6.2.0-system.txt
    pr_body: ""
  assert:
    - type: is-json

tests: file://test-cases-50-array.json

outputPath: ./results/model-comparison-final-results.json

evaluateOptions:
  maxConcurrency: 3
  showProgressBar: true
